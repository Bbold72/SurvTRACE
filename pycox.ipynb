{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from easydict import EasyDict\n",
    "from collections import defaultdict\n",
    "from sksurv.metrics import concordance_index_ipcw\n",
    "\n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "\n",
    "from pycox.models import PCHazard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "_ = torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from survtrace.dataset import load_data\n",
    "\n",
    "# define the setup parameters\n",
    "pc_hazard_config = EasyDict({\n",
    "    'data': 'metabric',\n",
    "    'horizons': [.25, .5, .75],\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 0.01,\n",
    "    'epochs': 50,\n",
    "    'hidden_size': 32\n",
    "})\n",
    "pc_hazard_config = EasyDict({\n",
    "    'data': 'support',\n",
    "    'horizons': [.25, .5, .75],\n",
    "    'batch_size': 128,\n",
    "    'learning_rate': 0.01,\n",
    "    'epochs': 50,\n",
    "    'hidden_size': 32\n",
    "})\n",
    "\n",
    "\n",
    "# load data\n",
    "df, df_train, df_y_train, df_test, df_y_test, df_val, df_y_val = load_data(pc_hazard_config)\n",
    "\n",
    "x_train = np.array(df_train, dtype='float32')\n",
    "x_val = np.array(df_val, dtype='float32')\n",
    "x_test = np.array(df_test, dtype='float32')\n",
    "\n",
    "y_df_to_tuple = lambda df: tuple([np.array(df['duration'], dtype='int64'), np.array(df['event'], dtype='float32'), np.array(df['proportion'], dtype='float32')])\n",
    "\n",
    "y_train = y_df_to_tuple(df_y_train)\n",
    "y_val = y_df_to_tuple(df_y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': 'support',\n",
       " 'horizons': [0.25, 0.5, 0.75],\n",
       " 'batch_size': 128,\n",
       " 'learning_rate': 0.01,\n",
       " 'epochs': 50,\n",
       " 'hidden_size': 32,\n",
       " 'labtrans': <survtrace.utils.LabelTransform at 0x7f97186872e8>,\n",
       " 'num_numerical_feature': 8,\n",
       " 'num_categorical_feature': 6,\n",
       " 'num_feature': 14,\n",
       " 'vocab_size': 25,\n",
       " 'duration_index': array([   0.  ,   14.  ,   57.  ,  250.25, 2029.  ]),\n",
       " 'out_feature': 4}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc_hazard_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = pc_hazard_config.hidden_size\n",
    "batch_norm = True\n",
    "dropout = 0.1\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(pc_hazard_config.num_feature, hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(hidden_size),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    \n",
    "    torch.nn.Linear(hidden_size, hidden_size),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.BatchNorm1d(hidden_size),\n",
    "    torch.nn.Dropout(0.1),\n",
    "    \n",
    "    torch.nn.Linear(hidden_size, pc_hazard_config.out_feature)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCHazard(net, tt.optim.Adam, duration_index=np.array(pc_hazard_config['duration_index'], dtype='float32'))\n",
    "model.optimizer.set_lr(pc_hazard_config.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\t[0s / 0s],\t\ttrain_loss: 1.6182,\tval_loss: 1.5877\n",
      "1:\t[0s / 0s],\t\ttrain_loss: 1.3642,\tval_loss: 1.4273\n",
      "2:\t[0s / 1s],\t\ttrain_loss: 1.3511,\tval_loss: 1.3700\n",
      "3:\t[0s / 1s],\t\ttrain_loss: 1.3372,\tval_loss: 1.3812\n",
      "4:\t[0s / 1s],\t\ttrain_loss: 1.3380,\tval_loss: 1.3655\n",
      "5:\t[0s / 2s],\t\ttrain_loss: 1.3348,\tval_loss: 1.3437\n",
      "6:\t[0s / 2s],\t\ttrain_loss: 1.3275,\tval_loss: 1.3512\n",
      "7:\t[0s / 3s],\t\ttrain_loss: 1.3289,\tval_loss: 1.3573\n",
      "8:\t[0s / 3s],\t\ttrain_loss: 1.3304,\tval_loss: 1.3643\n",
      "9:\t[0s / 3s],\t\ttrain_loss: 1.3257,\tval_loss: 1.3794\n",
      "10:\t[0s / 3s],\t\ttrain_loss: 1.3257,\tval_loss: 1.5841\n",
      "11:\t[0s / 4s],\t\ttrain_loss: 1.3235,\tval_loss: 1.3900\n",
      "12:\t[0s / 4s],\t\ttrain_loss: 1.3193,\tval_loss: 1.3515\n",
      "13:\t[0s / 4s],\t\ttrain_loss: 1.3189,\tval_loss: 1.3436\n",
      "14:\t[0s / 5s],\t\ttrain_loss: 1.3214,\tval_loss: 1.3449\n",
      "15:\t[0s / 5s],\t\ttrain_loss: 1.3251,\tval_loss: 1.3619\n",
      "16:\t[0s / 5s],\t\ttrain_loss: 1.3187,\tval_loss: 1.3492\n",
      "17:\t[0s / 6s],\t\ttrain_loss: 1.3187,\tval_loss: 1.4312\n",
      "18:\t[0s / 6s],\t\ttrain_loss: 1.3197,\tval_loss: 1.3921\n",
      "19:\t[0s / 6s],\t\ttrain_loss: 1.3115,\tval_loss: 1.3641\n",
      "20:\t[0s / 7s],\t\ttrain_loss: 1.3133,\tval_loss: 1.3579\n",
      "21:\t[0s / 7s],\t\ttrain_loss: 1.3194,\tval_loss: 1.3467\n",
      "22:\t[0s / 7s],\t\ttrain_loss: 1.3189,\tval_loss: 1.3679\n",
      "23:\t[0s / 7s],\t\ttrain_loss: 1.3166,\tval_loss: 1.3376\n",
      "24:\t[0s / 8s],\t\ttrain_loss: 1.3131,\tval_loss: 1.3407\n",
      "25:\t[0s / 8s],\t\ttrain_loss: 1.3086,\tval_loss: 1.3340\n",
      "26:\t[0s / 8s],\t\ttrain_loss: 1.3078,\tval_loss: 1.3891\n",
      "27:\t[0s / 9s],\t\ttrain_loss: 1.3126,\tval_loss: 1.3899\n",
      "28:\t[0s / 9s],\t\ttrain_loss: 1.3184,\tval_loss: 1.3670\n",
      "29:\t[0s / 9s],\t\ttrain_loss: 1.3120,\tval_loss: 1.3608\n",
      "30:\t[0s / 10s],\t\ttrain_loss: 1.3129,\tval_loss: 1.3342\n",
      "31:\t[0s / 10s],\t\ttrain_loss: 1.3168,\tval_loss: 1.3521\n",
      "32:\t[0s / 10s],\t\ttrain_loss: 1.3172,\tval_loss: 1.3523\n",
      "33:\t[0s / 11s],\t\ttrain_loss: 1.3190,\tval_loss: 1.3438\n",
      "34:\t[0s / 11s],\t\ttrain_loss: 1.3171,\tval_loss: 1.3467\n",
      "35:\t[0s / 11s],\t\ttrain_loss: 1.3133,\tval_loss: 1.3346\n"
     ]
    }
   ],
   "source": [
    "callbacks = [tt.callbacks.EarlyStopping()]\n",
    "log = model.fit(x_train, y_train, pc_hazard_config.batch_size, pc_hazard_config.epochs, callbacks, val_data=tuple([x_val, y_val]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self, df, train_index):\n",
    "        '''the input duration_train should be the raw durations (continuous),\n",
    "        NOT the discrete index of duration.\n",
    "        '''\n",
    "        self.df_train_all = df.loc[train_index]\n",
    "\n",
    "    def eval_single(self, model, test_set, config, val_batch_size=None):\n",
    "        df_train_all = self.df_train_all\n",
    "        get_target = lambda df: (df['duration'].values, df['event'].values)\n",
    "        durations_train, events_train = get_target(df_train_all)\n",
    "        print('durations_train', durations_train)\n",
    "        et_train = np.array([(events_train[i], durations_train[i]) for i in range(len(events_train))],\n",
    "                        dtype = [('e', bool), ('t', float)])\n",
    "        print('et_train', et_train)\n",
    "        times = config['duration_index'][1:-1]\n",
    "        print('times', times)\n",
    "        horizons = config['horizons']\n",
    "\n",
    "        df_test, df_y_test = test_set\n",
    "        surv = model.predict_surv_df(df_test, batch_size=val_batch_size)\n",
    "        risk = np.array((1 - surv).transpose())\n",
    "        print('risk', risk)\n",
    "        \n",
    "        durations_test, events_test = get_target(df_y_test)\n",
    "        print('durations_test', durations_test)\n",
    "        print('events_test', events_test)\n",
    "        et_test = np.array([(events_test[i], durations_test[i]) for i in range(len(events_test))],\n",
    "                    dtype = [('e', bool), ('t', float)])\n",
    "        print('et_test', et_test)\n",
    "        metric_dict = defaultdict(list)\n",
    "        cis = []\n",
    "        for i, _ in enumerate(times):\n",
    "            print('iteration', i)\n",
    "            print('risk', risk[:, i+1])\n",
    "            print(times)\n",
    "            cis.append(\n",
    "                concordance_index_ipcw(et_train, et_test, estimate=risk[:, i+1], tau=times[i])[0]\n",
    "                )\n",
    "            metric_dict[f'{horizons[i]}_ipcw'] = cis[i]\n",
    "\n",
    "\n",
    "        for horizon in enumerate(horizons):\n",
    "            print(f\"For {horizon[1]} quantile,\")\n",
    "            print(\"TD Concordance Index - IPCW:\", cis[horizon[0]])\n",
    "        \n",
    "        return metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "durations_train [ 30. 892.   7. ...  36.   6. 879.]\n",
      "et_train [( True,  30.) (False, 892.) ( True,   7.) ... ( True,  36.) ( True,   6.)\n",
      " (False, 879.)]\n",
      "times [ 14.    57.   250.25]\n",
      "risk [[0.         0.31434375 0.563365   0.7165142  0.85991776]\n",
      " [0.         0.22974098 0.41427994 0.50047255 0.6402138 ]\n",
      " [0.         0.19131464 0.34098452 0.46686047 0.71914124]\n",
      " ...\n",
      " [0.         0.20016217 0.3747046  0.51264817 0.86809427]\n",
      " [0.         0.1685158  0.35801423 0.733208   0.9536221 ]\n",
      " [0.         0.10061115 0.25156212 0.4210931  0.8292403 ]]\n",
      "durations_test [  31.  827.   79. ...  640.   51. 1388.]\n",
      "events_test [1 0 1 ... 0 1 1]\n",
      "et_test [( True,   31.) (False,  827.) ( True,   79.) ... (False,  640.)\n",
      " ( True,   51.) ( True, 1388.)]\n",
      "iteration 0\n",
      "risk [0.31434375 0.22974098 0.19131464 ... 0.20016217 0.1685158  0.10061115]\n",
      "[ 14.    57.   250.25]\n",
      "iteration 1\n",
      "risk [0.563365   0.41427994 0.34098452 ... 0.3747046  0.35801423 0.25156212]\n",
      "[ 14.    57.   250.25]\n",
      "iteration 2\n",
      "risk [0.7165142  0.50047255 0.46686047 ... 0.51264817 0.733208   0.4210931 ]\n",
      "[ 14.    57.   250.25]\n",
      "For 0.25 quantile,\n",
      "TD Concordance Index - IPCW: 0.6537429619072447\n",
      "For 0.5 quantile,\n",
      "TD Concordance Index - IPCW: 0.6216117828439424\n",
      "For 0.75 quantile,\n",
      "TD Concordance Index - IPCW: 0.6113766266416105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'0.25_ipcw': 0.6537429619072447,\n",
       "             '0.5_ipcw': 0.6216117828439424,\n",
       "             '0.75_ipcw': 0.6113766266416105})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = Evaluator(df, df_train.index)\n",
    "evaluator.eval_single(model, (x_test, df_y_test), config=pc_hazard_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
